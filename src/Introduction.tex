% Introduce to LLMs
% - Their advacement and how they are performing on SE benchmark. Mention code generation as one of the SE task.
% - Raise the research question.
Large language models (LLMs) have emerged as powerful tools in software engineering, offering substantial benefits to a multitude of programming tasks \cite{llm_application_se}. They can achieve state-of-the-art performance on several code benchmarks, demonstrating remarkable proficiency in areas like code generation, completion, and translation. \cite{roziere2023code}, \cite{hui2024qwen2}. 
%We've mentioned 2 code models here, should we stick to code models? Will this affect our model choice at all?
However, these advancements raise a fundamental question: Do LLMs truly generalize to new programming problems, or are they merely recalling memorized content that was seen in the training process? 

Prompting LLMs with queries in datasets that have been seen in the training process is considered a case of data contamination or leakage \cite{benchmark_cheater}. Memorization happens when the data was seen repeatedly in the training process such that the LLMs can generate it consistently \cite{emergent_memo} \cite{bordt2024elephants}. Memorization can be desirable or undesirable \cite{undesirablememorizationlargelanguage}. While desirable memorization can be beneficial for model performance \cite{memorizegeneralizeevaluatingllm}, undesirable memorization leads to transparency and auditing issues, making it difficult to distinguish whether specific outputs result from generalization or memorized content, and inflating performance on seen benchmark \cite{undesirablememorizationlargelanguage}.


% Introduce to the study of LLMs memorization in general and in software engineer task.
% - Recent research on LLMs memorization and their limitation:
% 	- Comparing outputs
% 	- Input mutation
% - Conclude that research on code translation is limited.

Recent research on memorization in LLMs was performed on the assumption that a dataset had been seen in the training process to unveil the effect of memorization by comparing the similarities between LLMs output and the training data \cite{mem_stackoverflow} \cite{demystifying} \cite{memorizegeneralizeevaluatingllm} \cite{unveiling_mem_in_code_models}%This last citation, is only for code models, not LLMs. Can we put it here still (The idea is correct)?
. Input data mutation was also investigated to understand memorization \cite{demystifying} \cite{memorizegeneralizeevaluatingllm}. However, the study of LLM memorization in code translation tasks is yet to be discussed.




% Our work
% - What will we do in this paper.
% - Briefly mention the result
In this paper, we measure the degree of memorization of LLMs in code translation tasks and study it's effect on the performance of the LLMs when there's no access to training data.

% Semantic Equivalence: Unlike general code generation (where correctness is measured by passing tests), code translation requires preserving the exact functional semantics while changing the syntax and structure. Memorization might help with syntax but fail on subtle functional differences.

% Token-Level vs. Functional Memorization: Most prior memorization work in code focused on verbatim token match. You should mention that code translation requires investigating functional memorizationâ€”did the model memorize the functionality of the source-target pair, not just the exact tokens?

% https://gemini.google.com/share/5dcbf06c66af
% Verbatim Memorization vs Functional Equivalence

% Our contribution
In summary, this paper makes the following contributions:
\begin{itemize}
	\item We did this
	\item We did that
	\item And we did one more thing
\end{itemize}
