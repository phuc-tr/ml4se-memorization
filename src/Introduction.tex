% Introduce to LLMs
% - Their advacement and how they are performing on SE benchmark. Mention code generation as one of the SE task.
% - Raise the research question.
Large language models (LLMs) have emerged as powerful tools in software engineering, offering substantial benefits to a multitude of programming tasks \cite{llm_application_se}. They can achieve state-of-the-art performance on several code benchmarks, demonstrating remarkable proficiency in areas like code generation, completion, and translation. \cite{roziere2023code}, \cite{hui2024qwen2}. 
%We've mentioned 2 code models here, should we stick to code models? Will this affect our model choice at all?
However, these advancements raise a fundamental question: Do LLMs truly generalize to new programming problems, or are they merely recalling memorized data that was seen in the training process \cite{xia2024leaderboardrankingcoding}? 



% Introduce to the study of LLMs memorization in general and in software engineer task.
% - Recent research on LLMs memorization and their limitation.
% - Conclude that research on code-code is limited.
Recent research measure memorization by ... <cite>. However, this assume access to training data, or access to the model... However, the study of undesired memorization of LLMs in code translation task is yet to be investigated.

% Our work
% - What will we do in this paper.
% - Briefly mention the result
In this paper, we measure the degree of memorization of LLMs in code translation tasks and study it's effect on the performance of the LLMs when there's no access to training data.

% Semantic Equivalence: Unlike general code generation (where correctness is measured by passing tests), code translation requires preserving the exact functional semantics while changing the syntax and structure. Memorization might help with syntax but fail on subtle functional differences.

% Token-Level vs. Functional Memorization: Most prior memorization work in code focused on verbatim token match. You should mention that code translation requires investigating functional memorizationâ€”did the model memorize the functionality of the source-target pair, not just the exact tokens?

% https://gemini.google.com/share/5dcbf06c66af
% Verbatim Memorization vs Functional Equivalence

% Our contribution
In summary, this paper makes the following contributions:
\begin{itemize}
	\item We did this
	\item We did that
	\item And we did one more thing
\end{itemize}
