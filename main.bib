@ARTICLE{llm_application_se,
	author={Ozkaya, Ipek},
	journal={IEEE Software}, 
	title={Application of Large Language Models to Software Engineering Tasks: Opportunities, Risks, and Implications}, 
	year={2023},
	volume={40},
	number={3},
	pages={4-8},
	keywords={},
	doi={10.1109/MS.2023.3248401}}

@article{roziere2023code,
	title={Code llama: Open foundation models for code},
	author={Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Sauvestre, Romain and Remez, Tal and others},
	journal={arXiv preprint arXiv:2308.12950},
	year={2023}
}

@article{hui2024qwen2,
	title={Qwen2. 5-coder technical report},
	author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Lu, Keming and others},
	journal={arXiv preprint arXiv:2409.12186},
	year={2024}
}

@article{satvaty2024undesirable,
	title={Undesirable memorization in large language models: A survey},
	author={Satvaty, Ali and Verberne, Suzan and Turkmen, Fatih},
	journal={arXiv preprint arXiv:2410.02650},
	year={2024}
}

@inproceedings{emergent_memo,
	author = {Biderman, Stella and Prashanth, USVSN Sai and Sutawika, Lintang and Schoelkopf, Hailey and Anthony, Quentin and Purohit, Shivanshu and Raff, Edward},
	title = {Emergent and predictable memorization in large language models},
	year = {2023},
	publisher = {Curran Associates Inc.},
	address = {Red Hook, NY, USA},
	abstract = {Memorization, or the tendency of large language models (LLMs) to output entire sequences from their training data verbatim, is a key concern for deploying language models. In particular, it is vital to minimize a model's memorization of sensitive datapoints such as those containing personal identifiable information (PII). The prevalence of such undesirable memorization can pose issues for model trainers, and may even require discarding an otherwise functional model. We therefore seek to predict which sequences will be memorized before a large model's full train-time by extrapolating the memorization behavior of lower-compute trial runs. We measure memorization in the Pythia model suite and plot scaling laws for forecasting memorization, allowing us to provide equi-compute recommendations to maximize the reliability (recall) of such predictions. We additionally provide further novel discoveries on the distribution of memorization scores across models and data. We release all code and data necessary to reproduce the results in this paper at https://github.com/EleutherAI/pythia.},
	booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
	articleno = {1219},
	numpages = {19},
	location = {New Orleans, LA, USA},
	series = {NIPS '23}
}

@inproceedings{
	bordt2024elephants,
	title={Elephants Never Forget: Memorization and Learning of Tabular Data in Large Language Models},
	author={Sebastian Bordt and Harsha Nori and Vanessa Cristiny Rodrigues Vasconcelos and Besmira Nushi and Rich Caruana},
	booktitle={First Conference on Language Modeling},
	year={2024},
	url={https://openreview.net/forum?id=HLoWN6m4fS}
}

@article{benchmark_cheater,
	title={Don't make your llm an evaluation benchmark cheater},
	author={Zhou, Kun and Zhu, Yutao and Chen, Zhipeng and Chen, Wentong and Zhao, Wayne Xin and Chen, Xu and Lin, Yankai and Wen, Ji-Rong and Han, Jiawei},
	journal={arXiv preprint arXiv:2311.01964},
	year={2023}
}

@article{
	mem_stackoverflow,
	title={Studying memorization of large language models using answers to Stack Overflow questions},
	author={Laura Caspari and Alexander Trautsch and Michael Granitzer and Steffen Herbold},
	journal={Transactions on Machine Learning Research},
	issn={2835-8856},
	year={2025},
	url={https://openreview.net/forum?id=ddocn44Kaq},
	note={}
}

@article{demystifying,
	author = {Kong, Jiaolong and Xie, Xiaofei and Liu, Shangqing},
	title = {Demystifying Memorization in LLM-Based Program Repair via a General Hypothesis Testing Framework},
	year = {2025},
	issue_date = {July 2025},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {2},
	number = {FSE},
	url = {https://doi.org/10.1145/3729390},
	doi = {10.1145/3729390},
	abstract = {Large Language Models (LLMs) have achieved remarkable success in various applications, particularly in code-related tasks such as code generation and program repair, setting new performance benchmarks. However, the extensive use of large training corpora raises concerns about whether these achievements stem from genuine understanding or mere memorization of training data—a question often overlooked in current research. This paper aims to study the memorization issue within LLM-based program repair by investigating whether the correct patches generated by LLMs are the result of memorization. The key challenge lies in the absence of ground truth for confirming memorization, leading to various ad-hoc methods designed for its detection. To address this challenge, we first propose a general framework that formalizes memorization detection as a general hypothesis testing problem, where existing approaches can be unified by defining a low-probability event under the null hypothesis that the data is not memorized. The occurrence of such an event leads to the rejection of the null hypothesis, indicating potential memorization. Based on this framework, we design two specific methods (i.e., low-probability events) to detect potential memorization: 1) basic ground-truth matching, and 2) reassessment after substantial code mutation. We investigate the memorization issue in LLM-based program repair using two datasets: Defects4J, a widely used benchmark that is likely included in the training data, and GitBug-Java, a new dataset that is unlikely to be part of the training data. Our findings reveal that a significant portion of correct patches exactly match the ground truths in Defects4J (e.g., 78.83\% and 87.42\% on GPT-3.5 and CodeLlama-7b, respectively). Moreover, even after significant modifications to the buggy code, where the original repairs should not be generated, a considerable percentage of bugs (e.g., 81.82\% on GPT-3.5 and 88.24\% on CodeLlama-7b) continue to be fixed exactly as in the original bug fixes, indicating a high likelihood of memorization. Furthermore, we evaluate existing memorization detection methods and demonstrate their ineffectiveness in this context (e.g., most AUROCs are below 0.5). The theoretical analysis under our hypothesis testing framework shows that their defined events may not meet the requirements for being low-probability. The study highlights the critical need for more robust and rigorous evaluations in LLM-based software engineering research, ensuring a clear distinction between true problem-solving capabilities and mere memorization.},
	journal = {Proc. ACM Softw. Eng.},
	month = jun,
	articleno = {FSE120},
	numpages = {23},
	keywords = {Code Memorization, Program Repair}
}

@misc{memorizegeneralizeevaluatingllm,
	title={Memorize or Generalize? Evaluating LLM Code Generation with Code Rewriting}, 
	author={Lizhe Zhang and Wentao Chen and Li Zhong and Letian Peng and Zilong Wang and Jingbo Shang},
	year={2025},
	eprint={2503.02296},
	archivePrefix={arXiv},
	primaryClass={cs.AI},
	url={https://arxiv.org/abs/2503.02296}, 
}

@inproceedings{memorize_performance, series={RANLP},
	title={PreCog: Exploring the Relation between Memorization and Performance in Pre-trained Language Models},
	url={http://dx.doi.org/10.26615/978-954-452-092-2_103},
	DOI={10.26615/978-954-452-092-2_103},
	booktitle={Proceedings of the Conference Recent Advances in Natural Language Processing - Large Language Models for Natural Language Processings},
	publisher={INCOMA Ltd., Shoumen, BULGARIA},
	author={Ranaldi, Leonardo and Ruzzetti, Elena Sofia and Massimo Zanzotto, Fabio},
	year={2023},
	pages={961–967},
	collection={RANLP} }

@misc{undesirablememorizationlargelanguage,
	title={Undesirable Memorization in Large Language Models: A Survey}, 
	author={Ali Satvaty and Suzan Verberne and Fatih Turkmen},
	year={2025},
	eprint={2410.02650},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2410.02650}, 
}